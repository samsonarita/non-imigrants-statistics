{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Non-immigrants' Statistics\n",
    "\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project \"Non-immigrants' Statistics\" collects data from different sources in order to determine the non-immigrants' trends in US states.    \n",
    "The data would give the ability to analyze the trends around non-immigrants while showing the influx and efflux of induviduals within the American states.       \n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#from pyspark.sql.functions import udf, desc, asc, sum\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType\n",
    "import datetime\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET_ACCESS_KEY']\n",
    "os.environ['S3_OUTPUT_DATA']=config['AWS']['SECRET_ACCESS_KEY']\n",
    "output_data = config['S3']['OUTPUT_DATA']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .appName(\"Non-immigrants' Trends\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "<!--#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc> -->\n",
    "\n",
    "\n",
    "##### Problem Statement\n",
    "\n",
    "In order to analyze the non-immigrants' trends, the bellow questions can be answered with the data:    \n",
    "1. Where do most U.S. non-immigrants visit/live?\n",
    "2. Who is arriving today? (By race/ethnicity/gender)\n",
    "3. Where do non-immigrants come from?\n",
    "4. How many people in the U.S. are non-immigrants? \n",
    "\n",
    "##### The choice of tools, technologies\n",
    "\n",
    "1. Spark (on an AWS - EMR Cluster) - To extract and transform the collected data from the data sources and write to S3 bucket.\n",
    "2. S3 bucket - To store the processed data model into parquet files of dimensions and fact tables\n",
    "3. Airflow (Deployed on AWS - E2 with a docker instance) - To create and run the ETL/ELT pipeline\n",
    "4. AWS Athena - To read processed data from S3 from dimensions and fact tables in parquet files for analysis\n",
    "\n",
    "<!--#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? -->\n",
    "\n",
    "##### Data Sources\n",
    "\n",
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.    \n",
    "- World Temperature Data: This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).    \n",
    "- U.S. City Demographic Data: This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).    \n",
    "- Airport Code Table: This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "##### Data Files\n",
    "\n",
    "- airport-codes_csv.csv - Airport Code Table    \n",
    "- I94_SAS_Labels_Descriptions.SAS - I94 Immigration Data (Labels Descriptions)    \n",
    "- us-cities-demographics.csv - U.S. City Demographic Data    \n",
    "- immigration_data_sample.csv - I94 Immigration Data (Data Sample with 1000 records)    \n",
    "- sas_data - I94 Immigration Data (In Parquet files) (Over 3 million records)    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport_codes_df=spark.read.csv(\"airport-codes_csv.csv\", header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_cities_demographics_df=spark.read.option(\"delimiter\", ';').csv(\"us-cities-demographics.csv\", header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_sample_df=spark.read.csv(\"immigration_data_sample.csv\", header='true', inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "<!-- #### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc. -->\n",
    "\n",
    "#### Cleaning Steps\n",
    "<!-- Document steps necessary to clean the data -->\n",
    "\n",
    " - The loaded data are from different formats, comma delimmited csv, semicolon delimitted csv and parquet file format    \n",
    " - The data is transformed to match the model fields e.g. extracting state information from another column\n",
    " - The data types to each field/column is updated match the required data types.\n",
    " - Null values are removed where required and unique values extracted where required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ident :  0\n",
      "type :  0\n",
      "name :  0\n",
      "continent :  0\n",
      "iso_country :  0\n",
      "iso_region :  0\n",
      "coordinates :  0\n",
      "state_code :  0\n",
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n",
      "22757\n",
      "+-----+-------------+-----------------+---------+-----------+----------+--------------------+----------+\n",
      "|ident|         type|             name|continent|iso_country|iso_region|         coordinates|state_code|\n",
      "+-----+-------------+-----------------+---------+-----------+----------+--------------------+----------+\n",
      "| 04LA|     heliport|St James Heliport|       NA|         US|     US-LA|-90.702222, 30.05...|        LA|\n",
      "| 05IA|small_airport|     Spotts Field|       NA|         US|     US-IA|-93.0682983398437...|        IA|\n",
      "+-----+-------------+-----------------+---------+-----------+----------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Extract States from airport codes\n",
    "airport_codes_df_ = airport_codes_df.withColumn('state_code', regexp_extract(col('iso_region'), '(US-)(\\w+)', 2))\n",
    "#Filter out rows with no state\n",
    "airport_codes_df_ = airport_codes_df_.filter((airport_codes_df_['state_code'].isNotNull())&(airport_codes_df_['state_code']!=''))\n",
    "airport_codes_df_.createOrReplaceTempView(\"airport_codes\")\n",
    "#Select columns for usage in model\n",
    "airport_codes = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            DISTINCT(ident),\n",
    "            type,\n",
    "            name,\n",
    "            continent,\n",
    "            iso_country,\n",
    "            iso_region,\n",
    "            coordinates,\n",
    "            state_code            \n",
    "        FROM airport_codes\n",
    "        \"\"\")\n",
    "\n",
    "#Confirm no colums with Nulls or Empty String\n",
    "for c in airport_codes.schema.names:\n",
    "    print(c, ': ',airport_codes.filter((airport_codes[c].isNull())|(airport_codes[c]=='')).count())\n",
    "    \n",
    "airport_codes.printSchema()\n",
    "print(airport_codes.count())    \n",
    "airport_codes.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city :  0\n",
      "state :  0\n",
      "med_age :  0\n",
      "total_pop :  0\n",
      "state_code :  0\n",
      "race :  0\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- med_age: integer (nullable = true)\n",
      " |-- total_pop: integer (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      "\n",
      "2891\n",
      "+----------+--------+-------+---------+----------+-----+\n",
      "|      city|   state|med_age|total_pop|state_code| race|\n",
      "+----------+--------+-------+---------+----------+-----+\n",
      "|     Flint|Michigan|     35|    98297|        MI|White|\n",
      "|Fort Worth|   Texas|     32|   836969|        TX|White|\n",
      "+----------+--------+-------+---------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select Columns for usage in model\n",
    "us_cities_demographics_df_ = us_cities_demographics_df.select(col(\"City\").alias(\"city\"),\n",
    "  col(\"State\").alias(\"state\"),\n",
    "  col(\"Median Age\").alias(\"med_age\"),\n",
    "  col(\"Total Population\").alias(\"total_pop\"),\n",
    "  col(\"State Code\").alias(\"state_code\"),\n",
    "  col(\"Race\").alias(\"race\")).select(['city','state','med_age','total_pop','state_code','race'])\n",
    "us_cities_demographics_df_.createOrReplaceTempView(\"us_cities_demographics\")\n",
    "#Select columns for usage in model\n",
    "us_cities_demographics = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            DISTINCT(city),\n",
    "            state,\n",
    "            CAST(med_age AS INT),\n",
    "            CAST(total_pop AS INT),\n",
    "            state_code,\n",
    "            race\n",
    "        FROM us_cities_demographics\n",
    "        \"\"\")\n",
    "\n",
    "for c in us_cities_demographics.schema.names:\n",
    "    print(c, ': ',us_cities_demographics.filter((us_cities_demographics[c].isNull())|(us_cities_demographics[c]=='')).count())\n",
    "\n",
    "us_cities_demographics.printSchema()\n",
    "print(us_cities_demographics.count())\n",
    "us_cities_demographics.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Capture US Cities. As null values exist on 194addr which tags cities to states, we will capture all cities tagged to states and use the same details on records with missing values\n",
    "notnull_cities_df = immigration_df.select(['i94port','i94addr']).filter(immigration_df['i94addr'].isNotNull()).groupBy(\"i94port\", \"i94addr\").agg(count('i94port').alias('count'))\n",
    "notnull_cities_df.createOrReplaceTempView(\"us_cities\")\n",
    "#Select columns for usage in model\n",
    "us_cities = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            DISTINCT(i94port) as city_code,\n",
    "            i94addr as state_code\n",
    "        FROM us_cities\n",
    "        \"\"\")\n",
    "\n",
    "for c in us_cities.schema.names:\n",
    "    print(c, ': ',us_cities.filter((us_cities[c].isNull())|(us_cities[c]=='')).count())\n",
    "\n",
    "us_cities.printSchema()\n",
    "print(us_cities.count())\n",
    "us_cities.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#non_immigrant_df_ = immigration_sample_df.select(['cicid','i94yr','i94mon','i94cit','i94res','i94port','arrdate','i94visa','count','admnum','visatype'])\n",
    "non_immigrant_df_ = immigration_sample_df.select(['admnum','i94bir','i94visa','dtadfile','visapost','occup','biryear','gender','insnum','visatype'])\n",
    "non_immigrant_df_.createOrReplaceTempView(\"non_immigrant\")\n",
    "#Select columns for usage in model\n",
    "non_immigrant = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            DISTINCT CAST(admnum AS BIGINT),\n",
    "            CAST (i94bir AS INT),\n",
    "            CAST (i94visa AS INT),\n",
    "            CAST (dtadfile AS INT),\n",
    "            visapost,\n",
    "            occup,\n",
    "            CAST (biryear AS INT),\n",
    "            gender,\n",
    "            insnum,\n",
    "            visatype           \n",
    "        FROM non_immigrant\n",
    "        \"\"\")\n",
    "\n",
    "#for c in non_immigrant.schema.names:\n",
    "#    print(c, ': ',non_immigrant.filter((non_immigrant[c].isNull())|(non_immigrant[c]=='')).count())\n",
    "\n",
    "non_immigrant.printSchema()\n",
    "print(non_immigrant.count())\n",
    "non_immigrant.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df_ = immigration_sample_df.select(['cicid','i94yr','i94mon','i94cit','i94res','i94port','arrdate','i94mode','i94addr','depdate','entdepa','entdepd','entdepu','matflag','dtaddto','airline','admnum','fltno'])\n",
    "\n",
    "immigration_df_.createOrReplaceTempView(\"immigration\")\n",
    "#Select columns for usage in model\n",
    "\n",
    "immigration = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            DISTINCT CAST(cicid AS INT),\n",
    "            CAST(i94yr AS INT),\n",
    "            CAST(i94mon AS INT),\n",
    "            CAST (i94cit AS INT),\n",
    "            CAST(i94res AS INT),\n",
    "            i94port as city_code,\n",
    "            CAST(arrdate AS INT),\n",
    "            CAST(i94mode AS INT),\n",
    "            i94addr as state_code,\n",
    "            CAST(depdate AS INT),\n",
    "            entdepa,\n",
    "            entdepd,\n",
    "            entdepu,\n",
    "            matflag,\n",
    "            CAST(dtaddto AS INT),\n",
    "            airline,\n",
    "            CAST(admnum AS BIGINT),\n",
    "            fltno\n",
    "        FROM immigration\n",
    "        \"\"\")\n",
    "\n",
    "immigration.printSchema()\n",
    "print(immigration.count())\n",
    "immigration.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "<!--Map out the conceptual data model and explain why you chose that model-->\n",
    "##### Dimension Tables\n",
    "\n",
    "    Table: airport_codes    \n",
    "     - Columns: ident,type,name,continent,iso_country,iso_region,coordinates,state_code    \n",
    "\n",
    "    Table: us_cities_demographics    \n",
    "     - Columns: city,state,med_age,total_pop,state_code,race\n",
    "\n",
    "    Table: us_cities    \n",
    "     - Columns: city_code,state_code\n",
    "\n",
    "    Table: non_immigrant    \n",
    "     - Columns: admnum,i94bir,i94visa,dtadfile,visapost,occup,biryear,gender,insnum,visatype\n",
    "\n",
    "##### Fact Tables\n",
    "\n",
    "    Table: immigration    \n",
    "     - Columns: cicid,i94yr,i94mon,i94cit,i94res,city_code,arrdate,i94mode,state_code,depdate,entdepa,entdepd,entdepu,matflag,dtaddto,airline,admnum,fltno\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "<!--List the steps necessary to pipeline the data into the chosen data model-->\n",
    "\n",
    "After the transformation of the data into the different dimensions and fact tables, the data would be written into s3 bucket in parquet files as follows:\n",
    "\n",
    " - Table: airport_codes  would be partitioned by state\n",
    " - Table: us_cities_demographics  would be partitioned by state\n",
    " - Table: us_cities would be partitioned by state \n",
    " - Table: non_immigrant would be partitioned by visatype  \n",
    " - Table: immigration would be partitioned by state(i94addr), city(i94port) and the arrival date(arrdate)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "<!--Build the data pipelines to create the data model.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport_codes.write.partitionBy(\"state_code\").parquet(output_data+\"airport_codes.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_cities_demographics.write.partitionBy(\"state_code\").parquet(output_data+\"us_cities_demographics.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_cities.write.partitionBy(\"state_code\").parquet(output_data+\"us_cities.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "non_immigrant.write.partitionBy(\"visatype\").parquet(output_data+\"non_immigrant.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration.write.partitionBy(\"state_code\",\"city_code\",\"arrdate\").parquet(output_data+\"immigration.parquet\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "<!--\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks\n",
    " -->\n",
    " 1. Loading each model file to spark and confirm data exists\n",
    " 2. Checking of change in size in the parquet files on s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Below is an example data quality check on checking if a file exists\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(output_data.split('/')[2])\n",
    "key = 'immigration.parquet'\n",
    "objs = list(bucket.objects.filter(Prefix=key))\n",
    "if len(objs) > 0 and objs[0].key == key:\n",
    "    print(\"Exists!\")\n",
    "else:\n",
    "    print(\"Doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Below is an example data quality check on loading a model file to spart and confirming if data exists\n",
    "\n",
    "checking_immigration =spark.read.parquet(output_data+\"immigration.parquet\")\n",
    "\n",
    "checking_immigration.createOrReplaceTempView(\"checking_immigration\")\n",
    "#Select columns for usage in model\n",
    "\n",
    "checking_immigration = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "           state_code\n",
    "        FROM checking_immigration LIMIT 1\n",
    "        \"\"\")\n",
    "print(checking_immigration.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "<!--Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.-->\n",
    "[Data Dictionary](data_dictionary.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "<!--* Clearly state the rationale for the choice of tools and technologies for the project. -->\n",
    "\n",
    "The rationale behind the choice of tools is to allow for scaling up in cases where a number of users are accessing the data and whenever updates are done more regularly.    \n",
    "\n",
    "##### The choice of tools, technologies\n",
    "\n",
    "1. Spark (on an AWS - EMR Cluster) - To extract and transform the collected data from the data sources and write to S3 bucket.\n",
    "2. S3 bucket - To store the processed data model into parquet files of dimensions and fact tables\n",
    "3. Airflow (Deployed on AWS - E2 with a docker instance) - To create and run the ETL/ELT pipeline\n",
    "4. AWS Athena - To read processed data from S3 from dimensions and fact tables in parquet files for analysis\n",
    "\n",
    "<!--* Propose how often the data should be updated and why.-->\n",
    "\n",
    "Data can be updated on a daily basis to allow for substantial data to be accumulated from the sources.      \n",
    "\n",
    "<!--* Write a description of how you would approach the problem differently under the following scenarios:-->\n",
    "<!-- * The data was increased by 100x.-->\n",
    "\n",
    "##### Problem scenarios\n",
    "\n",
    " - If data was increased by 100x, the scaling would be much easier as the cloud setup has already been thought through. This would allow for ease in scaling up.    \n",
    "<!-- * The data populates a dashboard that must be updated on a daily basis by 7am every day.-->\n",
    " - If the data populates a dashboard that must be updated on a daily basis by 7am every day, an airflow pipleine would be created with a DAG that updates regularly say, after every 6 hours in order to ensure there are atleast 3 tries within 24hours that would ensure that data has been populated and retried prior to deadline.    \n",
    "<!-- * The database needed to be accessed by 100+ people. -->\n",
    " - If the databases needed to be accessed by 100+ people, it would be much easiser to scale up as the cloud setup has already been thought through. This would allow for ease in scaling up.    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
